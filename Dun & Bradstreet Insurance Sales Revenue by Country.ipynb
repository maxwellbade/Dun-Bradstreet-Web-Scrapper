{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "China_pages = 187\n",
    "Spain_pages = 168\n",
    "Italy_pages = 72\n",
    "United_Kingdom_pages = 66\n",
    "Morocco_pages = 58\n",
    "United_States_pages = 52\n",
    "Colombia_pages = 47\n",
    "Israel_pages = 25\n",
    "Switzerland_pages = 12\n",
    "Finland_pages = 11\n",
    "Russian_Federation_pages = 10\n",
    "Uzbekistan_pages = 9\n",
    "Austria_pages = 7\n",
    "Hong_Kong_pages = 7\n",
    "Denmark_pages = 6\n",
    "Republic_Of_Korea_pages = 5\n",
    "Australia_pages = 5\n",
    "Belgium_pages = 4\n",
    "India_pages = 4\n",
    "Ireland_pages = 4\n",
    "Japan_pages = 3\n",
    "Singapore_pages = 3\n",
    "Germany_pages = 3\n",
    "Vietnam_pages = 2\n",
    "Portugal_pages = 2\n",
    "\n",
    "France_pages = 2\n",
    "Greece_pages = 2\n",
    "South_Africa_pages = 2\n",
    "Hungary_pages = 2\n",
    "Chile_pages = 2\n",
    "Bulgaria_pages = 2\n",
    "Canada_pages = 2\n",
    "Turkey_pages = 2\n",
    "\n",
    "Romania_pages = 2\n",
    "Croatia_pages = 2\n",
    "Estonia_pages = 2\n",
    "Philippines_pages = 2\n",
    "Serbia_pages = 2\n",
    "Luxembourg_pages = 2\n",
    "Bosnia_Herzegovina_pages = 2\n",
    "Nigeria_pages = 2\n",
    "Brazil_pages = 2\n",
    "\n",
    "Netherlands_pages = 2\n",
    "Saudi_Arabia_pages = 2\n",
    "Czech_Republic_pages = 2\n",
    "Slovenia_pages = 2\n",
    "United_Arab_Emirates_pages = 2\n",
    "Lebanon_pages = 2\n",
    "New_Zealand_pages = 2\n",
    "Liechtenstein_pages = 2\n",
    "Kenya_pages = 2\n",
    "\n",
    "Latvia_pages = 2\n",
    "Tunisia_pages = 2\n",
    "Argentina_pages = 2\n",
    "Bermuda_pages = 2\n",
    "Malaysia_pages = 2\n",
    "Panama_pages = 2\n",
    "Sweden_pages = 2\n",
    "\n",
    "Cyprus_pages = 2\n",
    "Ghana_pages = 2\n",
    "Iraq_pages = 2\n",
    "Mauritius_pages = 2\n",
    "Bahrain_pages = 2\n",
    "Egypt_pages = 2\n",
    "Thailand_pages = 2\n",
    "\n",
    "Bangladesh_pages = 2\n",
    "Ivory_Coast_pages = 2\n",
    "Indonesia_pages = 2\n",
    "Iceland_pages = 2\n",
    "Jordan_pages = 2\n",
    "Mozambique_pages = 2\n",
    "Poland_pages = 2\n",
    "Tanzania_pages = 2\n",
    "\n",
    "Cameroon_pages = 2\n",
    "Kuwait_pages = 2\n",
    "Libya_pages = 2\n",
    "Malta_pages = 2\n",
    "Mexico_pages = 2\n",
    "Oman_pages = 2\n",
    "Pakistan_pages = 2\n",
    "\n",
    "Uganda_pages = 2\n",
    "Venezuela_pages = 2\n",
    "Kosovo_pages = 2\n",
    "Angola_pages = 2\n",
    "Burkina_Faso_pages = 2\n",
    "Botswana_pages = 2\n",
    "\n",
    "Democratic_Republic_Of_The_Congo_pages = 2\n",
    "Guatemala_pages = 2\n",
    "Jamaica_pages = 2\n",
    "Cayman_Islands_pages = 2\n",
    "Madagascar_pages = 2\n",
    "\n",
    "Former_Yugoslav_Republic_Of_Macedonia_pages = 2\n",
    "Maldives_pages = 2\n",
    "Namibia_pages = 2\n",
    "Niger_pages = 2\n",
    "Peru_pages = 2\n",
    "\n",
    "Qatar_pages = 2\n",
    "Slovakia_pages = 2\n",
    "Togo_pages = 2\n",
    "Trinidad_and_Tobago_pages = 2\n",
    "Andorra_pages = 2\n",
    "Antigua_and_Barbuda_pages = 2\n",
    "Albania_pages = 2\n",
    "\n",
    "Barbados_pages = 2\n",
    "Benin_pages = 2\n",
    "Bolivia_pages = 2\n",
    "Bahamas_pages = 2\n",
    "Central_African_Republic_pages = 2\n",
    "Djibouti_pages = 2\n",
    "\n",
    "Algeria_pages = 2\n",
    "Ecuador_pages = 2\n",
    "Gibraltar_pages = 2\n",
    "Guam_pages = 2\n",
    "\n",
    "Haiti_pages = 2\n",
    "Sri_Lanka_pages = 2\n",
    "Macao_pages = 2\n",
    "Mauritania_pages = 2\n",
    "Nicaragua_pages = 2\n",
    "\n",
    "Nepal_pages = 2\n",
    "Rwanda_pages = 2\n",
    "El_Salvador_pages = 2\n",
    "Chad_pages = 2\n",
    "Taiwan_pages = 2\n",
    "British_Virgin_Islands_pages = 2\n",
    "\n",
    "#links\n",
    "China = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cn.html'\n",
    "Spain = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.es.html'\n",
    "Italy = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.it.html'\n",
    "United_Kingdom = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gb.html'\n",
    "Morocco = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ma.html'\n",
    "United_States = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.us.html'\n",
    "Colombia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.co.html'\n",
    "Israel = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.il.html'\n",
    "Switzerland = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ch.html'\n",
    "Finland = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.fi.html'\n",
    "Russian_Federation = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ru.html'\n",
    "Uzbekistan = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.uz.html'\n",
    "Austria = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.at.html'\n",
    "Hong_Kong = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.hk.html'\n",
    "Denmark = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.dk.html'\n",
    "Republic_Of_Korea = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.kr.html'\n",
    "Australia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.au.html'\n",
    "Belgium = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.be.html'\n",
    "India = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.in.html'\n",
    "Ireland = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ie.html'\n",
    "Japan = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.jp.html'\n",
    "Singapore = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.sg.html'\n",
    "Germany = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.de.html'\n",
    "Vietnam = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.vn.html'\n",
    "Portugal = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.pt.html'\n",
    "France = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.fr.html'\n",
    "Greece = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gr.html'\n",
    "South_Africa = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.za.html'\n",
    "Hungary = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.hu.html'\n",
    "Chile = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cl.html'\n",
    "Bulgaria = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bg.html'\n",
    "Canada = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ca.html'\n",
    "Turkey = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tr.html'\n",
    "Romania = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ro.html'\n",
    "Croatia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.hr.html'\n",
    "Estonia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ee.html'\n",
    "Philippines = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ph.html'\n",
    "Serbia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.rs.html'\n",
    "Luxembourg = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.lu.html'\n",
    "Bosnia_Herzegovina = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ba.html'\n",
    "Nigeria = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ng.html'\n",
    "Brazil = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.br.html'\n",
    "Netherlands = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.nl.html'\n",
    "Saudi_Arabia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.sa.html'\n",
    "Czech_Republic = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cz.html'\n",
    "Slovenia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.si.html'\n",
    "United_Arab_Emirates = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ae.html'\n",
    "Lebanon = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.lb.html'\n",
    "New_Zealand = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.nz.html'\n",
    "Liechtenstein = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.li.html'\n",
    "Kenya = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ke.html'\n",
    "Latvia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.lv.html'\n",
    "Tunisia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tn.html'\n",
    "Argentina = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ar.html'\n",
    "Bermuda = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bm.html'\n",
    "Malaysia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.my.html'\n",
    "Panama = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.pa.html'\n",
    "Sweden = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.se.html'\n",
    "Cyprus = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cy.html'\n",
    "Ghana = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gh.html'\n",
    "Iraq = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.iq.html'\n",
    "Mauritius = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mu.html'\n",
    "Bahrain = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bh.html'\n",
    "Egypt = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.eg.html'\n",
    "Thailand = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.th.html'\n",
    "Bangladesh = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bd.html'\n",
    "Ivory_Coast = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ci.html'\n",
    "Indonesia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.id.html'\n",
    "Iceland = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.is.html'\n",
    "Jordan = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.jo.html'\n",
    "\n",
    "Mozambique = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mz.html'\n",
    "Poland = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.pl.html'\n",
    "Tanzania = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tz.html'\n",
    "Cameroon = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cm.html'\n",
    "Kuwait = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.kw.html'\n",
    "Libya = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ly.html'\n",
    "Malta = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mt.html'\n",
    "Mexico = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mx.html'\n",
    "Oman = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.om.html'\n",
    "Pakistan = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.pk.html'\n",
    "\n",
    "Uganda = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ug.html'\n",
    "Venezuela = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ve.html'\n",
    "Kosovo = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.xk.html'\n",
    "Angola = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ao.html'\n",
    "Burkina_Faso = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bf.html'\n",
    "Botswana = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bw.html'\n",
    "Congo = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cd.html'\n",
    "\n",
    "Guatemala = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gt.html'\n",
    "Jamaica = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.jm.html'\n",
    "Cayman_Islands = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ky.html'\n",
    "Madagascar = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mg.html'\n",
    "\n",
    "Macedonia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mk.html'\n",
    "Maldives = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mv.html'\n",
    "Namibia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.na.html'\n",
    "Niger = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ne.html'\n",
    "Peru = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.pe.html'\n",
    "\n",
    "Qatar = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.qa.html'\n",
    "Slovakia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.sk.html'\n",
    "Togo = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tg.html'\n",
    "Trinidad_and_Tobago = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tt.html'\n",
    "Andorra = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ad.html'\n",
    "Antigua_and_Barbuda = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ag.html'\n",
    "Albania = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.al.html'\n",
    "\n",
    "Barbados = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bb.html'\n",
    "Benin = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bj.html'\n",
    "Bolivia = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bo.html'\n",
    "Bahamas = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.bs.html'\n",
    "Central_African = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cf.html'\n",
    "Djibouti = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.dj.html'\n",
    "\n",
    "Algeria = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.dz.html'\n",
    "Ecuador = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ec.html'\n",
    "Gibraltar = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gi.html'\n",
    "Guam = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gu.html'\n",
    "\n",
    "Haiti = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ht.html'\n",
    "Sri_Lanka = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.lk.html'\n",
    "Macao = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mo.html'\n",
    "Mauritania = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.mr.html'\n",
    "Nicaragua = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ni.html'\n",
    "\n",
    "Nepal = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.np.html'\n",
    "Rwanda = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.rw.html'\n",
    "El_Salvador = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.sv.html'\n",
    "Chad = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.td.html'\n",
    "Taiwan = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.tw.html'\n",
    "British_Virgin_Islands = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.vg.html'\n",
    "\n",
    "country_list = [#'China','Spain','Italy','United_Kingdom','Morocco',\n",
    "                'United_States','Colombia','Israel_ur','Switzerland','Finland','Russian_Federation','Uzbekistan','Austria','Hong_Kong','Denmark','Republic_Of_Korea','Australia','Belgium','India','Ireland','Japan','Singapore','Germany','Vietnam','Portugal','France','Greece','South_Africa','Hungary','Chile','Bulgaria','Canada','Turkey','Romania','Croatia','Estonia','Philippines','Serbia','Luxembourg','Bosnia_Herzegovina','Nigeria','Brazil','Netherlands','Saudi_Arabia','Czech_Republic','Slovenia','United_Arab_Emirates','Lebanon','New_Zealand','Liechtenstein','Kenya','Latvia','Tunisia','Argentina','Bermuda','Malaysia','Panama','Sweden','Cyprus','Ghana','Iraq','Mauritius','Bahrain','Egypt','Thailand','Bangladesh','Ivory_Coast','Indonesia','Iceland','Jordan','Mozambique','Poland','Tanzania','Cameroon','Kuwait','Libya','Malta','Mexico','Oman','Pakistan','Uganda','Venezuela','Kosovo','Angola','Burkina_Faso','Botswana','Congo','Guatemala','Jamaica','Cayman_Islands','Madagascar','Macedonia','Maldives','Namibia','Niger','Peru','Qatar','Slovakia','Togo','Trinidad_and_Tobago','Andorra','Antigua_and_Barbuda','Albania','Barbados','Benin','Bolivia','Bahamas','Central_African','Djibouti','Algeria','Ecuador','Gibraltar','Guam','Haiti','Sri_Lanka','Macao','Mauritania','Nicaragua','Nepal','Rwanda','El_Salvador','Chad','Taiwan','British_Virgin_Islands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#China\n",
    "country = 'China'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.cn.html'\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 2):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n",
      "Page 48...\n",
      "Page 49...\n",
      "Page 50...\n",
      "Page 51...\n",
      "Page 52...\n",
      "Page 53...\n",
      "Page 54...\n",
      "Page 55...\n",
      "Page 56...\n",
      "Page 57...\n",
      "Page 58...\n",
      "Page 59...\n",
      "Page 60...\n",
      "Page 61...\n",
      "Page 62...\n",
      "Page 63...\n",
      "Page 64...\n",
      "Page 65...\n",
      "Page 66...\n",
      "Page 67...\n",
      "Page 68...\n",
      "Page 69...\n",
      "Page 70...\n",
      "Page 71...\n",
      "Page 72...\n",
      "Page 73...\n",
      "Page 74...\n",
      "Page 75...\n",
      "Page 76...\n",
      "Page 77...\n",
      "Page 78...\n",
      "Page 79...\n",
      "Page 80...\n",
      "Page 81...\n",
      "Page 82...\n",
      "Page 83...\n",
      "Page 84...\n",
      "Page 85...\n",
      "Page 86...\n",
      "Page 87...\n",
      "Page 88...\n",
      "Page 89...\n",
      "Page 90...\n",
      "Page 91...\n",
      "Page 92...\n",
      "Page 93...\n",
      "Page 94...\n",
      "Page 95...\n",
      "Page 96...\n",
      "Page 97...\n",
      "Page 98...\n",
      "Page 99...\n",
      "Page 100...\n",
      "Page 101...\n",
      "Page 102...\n",
      "Page 103...\n",
      "Page 104...\n",
      "Page 105...\n",
      "Page 106...\n",
      "Page 107...\n",
      "Page 108...\n",
      "Page 109...\n",
      "Page 110...\n",
      "Page 111...\n",
      "Page 112...\n",
      "Page 113...\n",
      "Page 114...\n",
      "Page 115...\n",
      "Page 116...\n",
      "Page 117...\n",
      "Page 118...\n",
      "Page 119...\n",
      "Page 120...\n",
      "Page 121...\n",
      "Page 122...\n",
      "Page 123...\n",
      "Page 124...\n",
      "Page 125...\n",
      "Page 126...\n",
      "Page 127...\n",
      "Page 128...\n",
      "Page 129...\n",
      "Page 130...\n",
      "Page 131...\n",
      "Page 132...\n",
      "Page 133...\n",
      "Page 134...\n",
      "Page 135...\n",
      "Page 136...\n",
      "Page 137...\n",
      "Page 138...\n",
      "Page 139...\n",
      "Page 140...\n",
      "Page 141...\n",
      "Page 142...\n",
      "Page 143...\n",
      "Page 144...\n",
      "Page 145...\n",
      "Page 146...\n",
      "Page 147...\n",
      "Page 148...\n",
      "Page 149...\n",
      "Page 150...\n",
      "Page 151...\n",
      "Page 152...\n",
      "Page 153...\n",
      "Page 154...\n",
      "Page 155...\n",
      "Page 156...\n",
      "Page 157...\n",
      "Page 158...\n",
      "Page 159...\n",
      "Page 160...\n",
      "Page 161...\n",
      "Page 162...\n",
      "Page 163...\n",
      "Page 164...\n",
      "Page 165...\n",
      "Page 166...\n",
      "Page 167...\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "#Spain\n",
    "country = 'Spain'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.es.html'\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 168):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n",
      "Page 48...\n",
      "Page 49...\n",
      "Page 50...\n",
      "Page 51...\n",
      "Page 52...\n",
      "Page 53...\n",
      "Page 54...\n",
      "Page 55...\n",
      "Page 56...\n",
      "Page 57...\n",
      "Page 58...\n",
      "Page 59...\n",
      "Page 60...\n",
      "Page 61...\n",
      "Page 62...\n",
      "Page 63...\n",
      "Page 64...\n",
      "Page 65...\n",
      "Page 66...\n",
      "Page 67...\n",
      "Page 68...\n",
      "Page 69...\n",
      "Page 70...\n",
      "Page 71...\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "#Italy\n",
    "country = 'Italy'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.it.html'\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 72):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n",
      "Page 48...\n",
      "Page 49...\n",
      "Page 50...\n",
      "Page 51...\n",
      "Page 52...\n",
      "Page 53...\n",
      "Page 54...\n",
      "Page 55...\n",
      "Page 56...\n",
      "Page 57...\n",
      "Page 58...\n",
      "Page 59...\n",
      "Page 60...\n",
      "Page 61...\n",
      "Page 62...\n",
      "Page 63...\n",
      "Page 64...\n",
      "Page 65...\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "#UK\n",
    "country = 'UK'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.gb.html'\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 66):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n",
      "Page 48...\n",
      "Page 49...\n",
      "Page 50...\n",
      "Page 51...\n",
      "Page 52...\n",
      "Page 53...\n",
      "Page 54...\n",
      "Page 55...\n",
      "Page 56...\n",
      "Page 57...\n",
      "Page 58...\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "#Morocco\n",
    "country = 'Morocco'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = 'https://www.dnb.com/business-directory/company-information.insurance-carriers.ma.html'\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 59):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n",
      "Page 48...\n",
      "Page 49...\n",
      "Page 50...\n",
      "Page 51...\n",
      "Page 52...\n"
     ]
    }
   ],
   "source": [
    "#United_States\n",
    "country = 'United_States'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = United_States\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 53):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n",
      "Page 25...\n",
      "Page 26...\n",
      "Page 27...\n",
      "Page 28...\n",
      "Page 29...\n",
      "Page 30...\n",
      "Page 31...\n",
      "Page 32...\n",
      "Page 33...\n",
      "Page 34...\n",
      "Page 35...\n",
      "Page 36...\n",
      "Page 37...\n",
      "Page 38...\n",
      "Page 39...\n",
      "Page 40...\n",
      "Page 41...\n",
      "Page 42...\n",
      "Page 43...\n",
      "Page 44...\n",
      "Page 45...\n",
      "Page 46...\n",
      "Page 47...\n"
     ]
    }
   ],
   "source": [
    "#Colombia\n",
    "country = 'Colombia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Colombia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 48):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n",
      "Page 12...\n",
      "Page 13...\n",
      "Page 14...\n",
      "Page 15...\n",
      "Page 16...\n",
      "Page 17...\n",
      "Page 18...\n",
      "Page 19...\n",
      "Page 20...\n",
      "Page 21...\n",
      "Page 22...\n",
      "Page 23...\n",
      "Page 24...\n"
     ]
    }
   ],
   "source": [
    "#Israel\n",
    "country = 'Israel'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Israel\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 25):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n"
     ]
    }
   ],
   "source": [
    "#Switzerland\n",
    "country = 'Switzerland'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Switzerland\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 12):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n",
      "Page 10...\n",
      "Page 11...\n"
     ]
    }
   ],
   "source": [
    "#Finland\n",
    "country = 'Finland'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Finland\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 12):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n",
      "Page 9...\n"
     ]
    }
   ],
   "source": [
    "#Russian_Federation\n",
    "country = 'Russian_Federation'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Russian_Federation\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 10):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n",
      "Page 7...\n",
      "Page 8...\n"
     ]
    }
   ],
   "source": [
    "#Uzbekistan\n",
    "country = 'Uzbekistan'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Uzbekistan\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, 9):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n"
     ]
    }
   ],
   "source": [
    "#Austria\n",
    "country = 'Austria'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Austria\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Austria_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n",
      "Page 6...\n"
     ]
    }
   ],
   "source": [
    "#Hong_Kong\n",
    "country = 'Hong_Kong'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Hong_Kong\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Hong_Kong_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n",
      "Page 5...\n"
     ]
    }
   ],
   "source": [
    "#Denmark\n",
    "country = 'Denmark'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Denmark\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Denmark_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n"
     ]
    }
   ],
   "source": [
    "#Republic_Of_Korea\n",
    "country = 'Republic_Of_Korea'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Republic_Of_Korea\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Republic_Of_Korea_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n",
      "Page 4...\n"
     ]
    }
   ],
   "source": [
    "#Australia\n",
    "country = 'Australia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Australia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Australia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n"
     ]
    }
   ],
   "source": [
    "#Belgium\n",
    "country = 'Belgium'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Belgium\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Belgium_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n"
     ]
    }
   ],
   "source": [
    "#India\n",
    "country = 'India'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = India\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, India_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n",
      "Page 3...\n"
     ]
    }
   ],
   "source": [
    "#Ireland\n",
    "country = 'Ireland'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Ireland\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Ireland_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n"
     ]
    }
   ],
   "source": [
    "#Japan\n",
    "country = 'Japan'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Japan\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Japan_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n"
     ]
    }
   ],
   "source": [
    "#Singapore\n",
    "country = 'Singapore'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Singapore\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Singapore_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n",
      "Page 2...\n"
     ]
    }
   ],
   "source": [
    "#Germany\n",
    "country = 'Germany'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Germany\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Germany_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Vietnam\n",
    "country = 'Vietnam'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Vietnam\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Vietnam_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Portugal\n",
    "country = 'Portugal'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Portugal\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Portugal_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#France\n",
    "country = 'France'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = France\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, France_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Greece\n",
    "country = 'Greece'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Greece\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Greece_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#South_Africa\n",
    "country = 'South_Africa'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = South_Africa\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, South_Africa_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Hungary\n",
    "country = 'Hungary'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Hungary\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Hungary_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Chile\n",
    "country = 'Chile'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Chile\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Chile_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bulgaria\n",
    "country = 'Bulgaria'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bulgaria\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bulgaria_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Canada\n",
    "country = 'Canada'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Canada\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Canada_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Turkey\n",
    "country = 'Turkey'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Turkey\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Turkey_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Romania\n",
    "country = 'Romania'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Romania\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Romania_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Croatia\n",
    "country = 'Croatia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Croatia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Croatia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Estonia\n",
    "country = 'Estonia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Estonia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Estonia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Philippines\n",
    "country = 'Philippines'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Philippines\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Philippines_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Serbia\n",
    "country = 'Serbia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Serbia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Serbia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Luxembourg\n",
    "country = 'Luxembourg'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Luxembourg\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Luxembourg_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bosnia_Herzegovina\n",
    "country = 'Bosnia_Herzegovina'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bosnia_Herzegovina\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bosnia_Herzegovina_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Nigeria\n",
    "country = 'Nigeria'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Nigeria\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Nigeria_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Brazil\n",
    "country = 'Brazil'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Brazil\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Brazil_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Netherlands\n",
    "country = 'Netherlands'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Netherlands\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Netherlands_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Saudi_Arabia\n",
    "country = 'Saudi_Arabia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Saudi_Arabia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Saudi_Arabia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Czech_Republic\n",
    "country = 'Czech_Republic'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Czech_Republic\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Czech_Republic_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Slovenia\n",
    "country = 'Slovenia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Slovenia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Slovenia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#United_Arab_Emirates\n",
    "country = 'United_Arab_Emirates'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = United_Arab_Emirates\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, United_Arab_Emirates_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Lebanon\n",
    "country = 'Lebanon'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Lebanon\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Lebanon_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#New_Zealand\n",
    "country = 'New_Zealand'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = New_Zealand\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, New_Zealand_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Liechtenstein\n",
    "country = 'Liechtenstein'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Liechtenstein\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Liechtenstein_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Kenya\n",
    "country = 'Kenya'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Kenya\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Kenya_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Latvia\n",
    "country = 'Latvia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Latvia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Latvia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Tunisia\n",
    "country = 'Tunisia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Tunisia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Tunisia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Argentina\n",
    "country = 'Argentina'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Argentina\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Argentina_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bermuda\n",
    "country = 'Bermuda'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bermuda\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bermuda_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Malaysia\n",
    "country = 'Malaysia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Malaysia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Malaysia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Panama\n",
    "country = 'Panama'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Panama\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Panama_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Sweden\n",
    "country = 'Sweden'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Sweden\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Sweden_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Cyprus\n",
    "country = 'Cyprus'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Cyprus\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Cyprus_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Ghana\n",
    "country = 'Ghana'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Ghana\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Ghana_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Iraq\n",
    "country = 'Iraq'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Iraq\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Iraq_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Mauritius\n",
    "country = 'Mauritius'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Mauritius\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Mauritius_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bahrain\n",
    "country = 'Bahrain'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bahrain\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bahrain_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Egypt\n",
    "country = 'Egypt'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Egypt\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Egypt_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Thailand\n",
    "country = 'Thailand'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Thailand\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Thailand_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bangladesh\n",
    "country = 'Bangladesh'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bangladesh\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bangladesh_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Ivory_Coast\n",
    "country = 'Ivory_Coast'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Ivory_Coast\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Ivory_Coast_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Indonesia\n",
    "country = 'Indonesia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Indonesia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Indonesia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Iceland\n",
    "country = 'Iceland'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Iceland\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Iceland_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Jordan\n",
    "country = 'Jordan'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Jordan\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Jordan_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Mozambique\n",
    "country = 'Mozambique'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Mozambique\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Mozambique_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Poland\n",
    "country = 'Poland'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Poland\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Poland_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Tanzania\n",
    "country = 'Tanzania'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Tanzania\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Tanzania_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Cameroon\n",
    "country = 'Cameroon'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Cameroon\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Cameroon_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Kuwait\n",
    "country = 'Kuwait'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Kuwait\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Kuwait_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Libya\n",
    "country = 'Libya'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Libya\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Libya_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Malta\n",
    "country = 'Malta'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Malta\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Malta_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Mexico\n",
    "country = 'Mexico'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Mexico\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Mexico_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Oman\n",
    "country = 'Oman'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Oman\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Oman_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Pakistan\n",
    "country = 'Pakistan'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Pakistan\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Pakistan_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Uganda\n",
    "country = 'Uganda'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Uganda\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Uganda_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Venezuela\n",
    "country = 'Venezuela'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Venezuela\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Venezuela_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Kosovo\n",
    "country = 'Kosovo'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Kosovo\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Kosovo_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Angola\n",
    "country = 'Angola'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Angola\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Angola_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Burkina_Faso\n",
    "country = 'Burkina_Faso'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Burkina_Faso\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Burkina_Faso_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Botswana\n",
    "country = 'Botswana'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Botswana\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Botswana_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Congo\n",
    "country = 'Congo'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Congo\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Democratic_Republic_Of_The_Congo_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Guatemala\n",
    "country = 'Guatemala'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Guatemala\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Guatemala_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Jamaica\n",
    "country = 'Jamaica'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Jamaica\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Jamaica_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Cayman_Islands\n",
    "country = 'Cayman_Islands'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Cayman_Islands\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Cayman_Islands_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Madagascar\n",
    "country = 'Madagascar'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Madagascar\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Madagascar_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Macedonia\n",
    "country = 'Macedonia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Macedonia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Former_Yugoslav_Republic_Of_Macedonia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Maldives\n",
    "country = 'Maldives'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Maldives\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Maldives_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Namibia\n",
    "country = 'Namibia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Namibia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Namibia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Niger\n",
    "country = 'Niger'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Niger\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Niger_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Peru\n",
    "country = 'Peru'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Peru\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Peru_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Qatar\n",
    "country = 'Qatar'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Qatar\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Qatar_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Slovakia\n",
    "country = 'Slovakia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Slovakia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Slovakia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Togo\n",
    "country = 'Togo'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Togo\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Togo_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Trinidad_and_Tobago\n",
    "country = 'Trinidad_and_Tobago'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Trinidad_and_Tobago\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Trinidad_and_Tobago_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Andorra\n",
    "country = 'Andorra'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Andorra\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Andorra_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Antigua_and_Barbuda\n",
    "country = 'Antigua_and_Barbuda'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Antigua_and_Barbuda\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Antigua_and_Barbuda_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Albania\n",
    "country = 'Albania'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Albania\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Albania_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Barbados\n",
    "country = 'Barbados'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Barbados\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Barbados_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Benin\n",
    "country = 'Benin'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Benin\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Benin_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bolivia\n",
    "country = 'Bolivia'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bolivia\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bolivia_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Bahamas\n",
    "country = 'Bahamas'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Bahamas\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Bahamas_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Central_African\n",
    "country = 'Central_African'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Central_African\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Central_African_Republic_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Djibouti\n",
    "country = 'Djibouti'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Djibouti\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Djibouti_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Algeria\n",
    "country = 'Algeria'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Algeria\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Algeria_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Ecuador\n",
    "country = 'Ecuador'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Ecuador\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Ecuador_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Gibraltar\n",
    "country = 'Gibraltar'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Gibraltar\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Gibraltar_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Guam\n",
    "country = 'Guam'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Guam\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Guam_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Haiti\n",
    "country = 'Haiti'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Haiti\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Haiti_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Sri_Lanka\n",
    "country = 'Sri_Lanka'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Sri_Lanka\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Sri_Lanka_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Macao\n",
    "country = 'Macao'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Macao\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Macao_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Mauritania\n",
    "country = 'Mauritania'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Mauritania\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Mauritania_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Nicaragua\n",
    "country = 'Nicaragua'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Nicaragua\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Nicaragua_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Nepal\n",
    "country = 'Nepal'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Nepal\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Nepal_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Rwanda\n",
    "country = 'Rwanda'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Rwanda\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Rwanda_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#El_Salvador\n",
    "country = 'El_Salvador'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = El_Salvador\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, El_Salvador_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Chad\n",
    "country = 'Chad'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Chad\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Chad_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#Taiwan\n",
    "country = 'Taiwan'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = Taiwan\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, Taiwan_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1...\n"
     ]
    }
   ],
   "source": [
    "#British_Virgin_Islands\n",
    "country = 'British_Virgin_Islands'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "params = {'page': 1}\n",
    "url = British_Virgin_Islands\n",
    "\n",
    "all_data = []\n",
    "for params['page'] in range(1, British_Virgin_Islands_pages):  # <-- increase number of pages here\n",
    "    print('Page {}...'.format(params['page']))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers, params=params).content, 'html.parser')\n",
    "\n",
    "    # remove unnecessary information:\n",
    "    for t in soup.select('.show-mobile'):\n",
    "        t.extract()\n",
    "\n",
    "    for c1, c2, c3 in zip(soup.select('#companyResults .col-md-6')[1:],\n",
    "                        soup.select('#companyResults .col-md-4')[1:],\n",
    "                        soup.select('#companyResults .col-md-2')[1:]):\n",
    "        all_data.append({\n",
    "            'Name': c1.get_text(strip=True),\n",
    "            'Location': ' '.join(c2.get_text(strip=True).split()),\n",
    "            'Revenue': c3.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(country+'.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-470-59dd7ff90004>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Revenue'] = df['Revenue'].str.replace(r'M', '')\n",
      "<ipython-input-470-59dd7ff90004>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Revenue'] = df['Revenue'].str.replace(r'$', '')\n",
      "<ipython-input-470-59dd7ff90004>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Revenue'] = df['Revenue'].str.replace(r',', '')\n",
      "<ipython-input-470-59dd7ff90004>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Revenue'] = df.Revenue.astype(float)\n"
     ]
    }
   ],
   "source": [
    "#merge all csvs\n",
    "path = r'C:\\Users\\maxwell.bade\\Downloads\\Highmark\\andi\\test'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame.to_csv('insurance_rev_country.csv')\n",
    "\n",
    "#keep only interesting columns\n",
    "df = frame[['Name','Location','Revenue']]\n",
    "\n",
    "#remove unwanted characters\n",
    "df['Revenue'] = df['Revenue'].str.replace(r'M', '')\n",
    "df['Revenue'] = df['Revenue'].str.replace(r'$', '')\n",
    "df['Revenue'] = df['Revenue'].str.replace(r',', '')\n",
    "df['Revenue'] = df.Revenue.astype(float)\n",
    "\n",
    "#add country column\n",
    "df['Country'] = df['Location'].str.rsplit(',').str[-1]\n",
    "\n",
    "#rename cols\n",
    "df.columns = ['Name','Location','Revenue(in Millions)','Country']\n",
    "\n",
    "df.to_csv('insurance_rev_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Revenue(in Millions)</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SIGAL UNIQA GROUP AUSTRIA SH.A.</td>\n",
       "      <td>Tirane,Albania</td>\n",
       "      <td>74.225236</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMPAGNIE ALGERIENNE DASSURANCE</td>\n",
       "      <td>Alger-centre, Wilaya D Alger,Algeria</td>\n",
       "      <td>-40.056823</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MORA ASSEGURANCES SAU</td>\n",
       "      <td>Escaldes-engordany,Andorra</td>\n",
       "      <td>0.154892</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIC SEGUROS SA</td>\n",
       "      <td>Luanda,Angola</td>\n",
       "      <td>15.309966</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIDELIDADE ANGOLA</td>\n",
       "      <td>Luanda,Angola</td>\n",
       "      <td>3.798560</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38464</th>\n",
       "      <td>PETROLIMEX INSURANCE CORPORATION- BRANCH IN TA...</td>\n",
       "      <td>Tay Ninh, Tay Ninh,Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38465</th>\n",
       "      <td>PETROLIMEX INSURANCE CORPORATION- BRANCH IN KH...</td>\n",
       "      <td>Nha Trang, Khanh Hoa,Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38466</th>\n",
       "      <td>PETROLIMEX INSURANCE CORPORATION- BRANCH IN KI...</td>\n",
       "      <td>Rach Gia, Kien Giang,Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38467</th>\n",
       "      <td>PETROLIMEX INSURANCE CORPORATION- BRANCH IN TU...</td>\n",
       "      <td>Tuyen Quang, Tuyen Quang,Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38468</th>\n",
       "      <td>PETROLIMEX INSURANCE CORPORATION- BRANCH IN BI...</td>\n",
       "      <td>Quy Nhon, Binh Dinh,Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38469 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Name                              Location  Revenue(in Millions)  Country\n",
       "0                        SIGAL UNIQA GROUP AUSTRIA SH.A.                        Tirane,Albania             74.225236  Albania\n",
       "1                        COMPAGNIE ALGERIENNE DASSURANCE  Alger-centre, Wilaya D Alger,Algeria            -40.056823  Algeria\n",
       "2                                  MORA ASSEGURANCES SAU            Escaldes-engordany,Andorra              0.154892  Andorra\n",
       "3                                         BIC SEGUROS SA                         Luanda,Angola             15.309966   Angola\n",
       "4                                      FIDELIDADE ANGOLA                         Luanda,Angola              3.798560   Angola\n",
       "...                                                  ...                                   ...                   ...      ...\n",
       "38464  PETROLIMEX INSURANCE CORPORATION- BRANCH IN TA...            Tay Ninh, Tay Ninh,Vietnam                   NaN  Vietnam\n",
       "38465  PETROLIMEX INSURANCE CORPORATION- BRANCH IN KH...          Nha Trang, Khanh Hoa,Vietnam                   NaN  Vietnam\n",
       "38466  PETROLIMEX INSURANCE CORPORATION- BRANCH IN KI...          Rach Gia, Kien Giang,Vietnam                   NaN  Vietnam\n",
       "38467  PETROLIMEX INSURANCE CORPORATION- BRANCH IN TU...      Tuyen Quang, Tuyen Quang,Vietnam                   NaN  Vietnam\n",
       "38468  PETROLIMEX INSURANCE CORPORATION- BRANCH IN BI...           Quy Nhon, Binh Dinh,Vietnam                   NaN  Vietnam\n",
       "\n",
       "[38469 rows x 4 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
